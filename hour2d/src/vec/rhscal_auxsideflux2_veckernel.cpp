//
// auto-generated by op2.py
//

//user function
inline void rhscal_auxsideflux2(side *psid, side *pbsesid, const uvar *pvar0, const uvar *pvar1, const elem *pele0, const elem *pele1)
{	
		int bceR = (*psid).ofElem[0];
		if( bceR >= 0 )
		{
			printf( "calauxsideflux bc error!\n" );
	//		getchar();    exit(1);
		}
		 int  nbctype = GetBCType( bceR );
//		int eL  = (*psid).ofElem[1];
		int tsL = (*psid).lsdidx[1];	
		 int  nfvordL = (*pvar0).pfv;
		 int  nfvdofL = DOFofOrd2d[nfvordL];
		 int   nTypeL = ELEMTYPE;
#ifdef QUAD
		 const int * inodeL = (*pele0).Node;
		if( inodeL[3]==inodeL[0] ) nTypeL = 3;
#endif
		double norm[NDIM];
		for(  int  nd=0; nd<NDIM; nd++ )
			norm[nd] = (*psid).norm[nd];
		 int  nqdpt = (*psid).nqdpt;
		double (*SdQdPtBasL)[MAXFVDOF]; 
		if( nTypeL==3 ) SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasTri [tsL+3][nqdpt];
		else            SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasRect[tsL+4][nqdpt];
		// each gauss quad pts
		for(  int  qd = 0; qd < nqdpt; ++qd )
		{
			double tcvL[NEQ];
			GetVar( nfvdofL, (*pvar0).wh, SdQdPtBasL[qd], tcvL );
			double tcvR[NEQ];
			if( ( nbctype==BC_SOLIDSURFACE )||( nbctype==BC_GENERIC1 ) )
			{
				bcvisSolidSurf( tcvL, (*pvar0).wh[0], &param, tcvR );
			}
			else if( nbctype==BC_FARFIELD )
			{
				bcFarField( tcvL, (*pvar0).wh[0], norm, &param, tcvR );
			}
			else if( nbctype==BC_SYMMETRY )
			{
				bcSymmetry( tcvL, (*pvar0).wh[0], norm, tcvR );
			}
			else if( nbctype==BC_INTERBLK )
			{
//				 int  eR = GetElement( bceR );
				int tsR = (*psid).lsdidx[0];	
				 int  nfvordR = (*pvar1).pfv;
				 int  nfvdofR = DOFofOrd2d[nfvordR];
				 int  qdposR = qd;
				 int   nTypeR = ELEMTYPE;
#ifdef QUAD
				 const int * inodeR = (*pele1).Node;
				if( inodeR[3]==inodeR[0] ) nTypeR = 3;
#endif
				if( nTypeR==3 ) 
					GetVar( nfvdofR, (*pvar1).wh, SdQdPtBasTri [tsR][nqdpt][qdposR], tcvR );
				else // rect
					GetVar( nfvdofR, (*pvar1).wh, SdQdPtBasRect[tsR][nqdpt][qdposR], tcvR );

				double* tnbflux = (*pbsesid).flux[nqdpt-1-qd];
#if		AUXVARIABLE==CONSERV
				for(  int  j=0; j<NEQ; j++ )
				{
					(*psid).flux[qd][j] = 0.5*(tcvL[j] + tcvR[j]);
					tnbflux[j] = (*psid).flux[qd][j];
				}
#elif	AUXVARIABLE==PRIMITI
				double tpvL[NEQ];
				CV2PV( tcvL, (*pvar0).wh[0], tpvL );
				double tpvR[NEQ];
				CV2PV( tcvR, (*pvar1).wh[0], tpvR );
				tpvL[NEQ-1] /= tpvL[0]*param.R;
				tpvR[NEQ-1] /= tpvR[0]*param.R;
				for(  int  j=0; j<NEQ; j++ )
				{
					(*psid).flux[qd][j] = 0.5*(tpvL[j] + tpvR[j]);
					tnbflux[j] = (*psid).flux[qd][j];
				}
#endif
				continue;// next quad pts
			}

#if		AUXVARIABLE==CONSERV
			for(  int  j=0; j<NEQ; j++ )
			{
				(*psid).flux[qd][j] = tcvR[j];
			}
#elif	AUXVARIABLE==PRIMITI
			double tpvR[NEQ];
			CV2PV( tcvR, tcvR, tpvR );
			tpvR[NEQ-1] /= tpvR[0]*param.R;
			for(  int  j=0; j<NEQ; j++ )
			{
				(*psid).flux[qd][j] = tpvR[j];
			}
#endif
			continue;// next quad pts
		}// end of no pts
}
#ifdef VECTORIZE
//user function -- modified for vectorisation
#if defined __clang__ || defined __GNUC__
__attribute__((always_inline))
#endif
inline void rhscal_auxsideflux2_vec( side psid[][SIMD_VEC], side pbsesid[][SIMD_VEC], const uvar pvar0[][SIMD_VEC], const uvar pvar1[][SIMD_VEC], const elem pele0[][SIMD_VEC], const elem pele1[][SIMD_VEC], int idx ) {
		int bceR = (psid[0][idx]).ofElem[0];
		if( bceR >= 0 )
		{
			printf( "calauxsideflux bc error!\n" );

		}
		 int  nbctype = GetBCType( bceR );

		int tsL = (psid[0][idx]).lsdidx[1];
		 int  nfvordL = (pvar0[0][idx]).pfv;
		 int  nfvdofL = DOFofOrd2d[nfvordL];
		 int   nTypeL = ELEMTYPE;
#ifdef QUAD
		 const int * inodeL = (pele0[0][idx]).Node;
		if( inodeL[3]==inodeL[0] ) nTypeL = 3;
#endif
		double norm[NDIM];
		for(  int  nd=0; nd<NDIM; nd++ )
			norm[nd] = (psid[0][idx]).norm[nd];
		 int  nqdpt = (psid[0][idx]).nqdpt;
		double (*SdQdPtBasL)[MAXFVDOF];
		if( nTypeL==3 ) SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasTri [tsL+3][nqdpt];
		else            SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasRect[tsL+4][nqdpt];

		for(  int  qd = 0; qd < nqdpt; ++qd )
		{
			double tcvL[NEQ];
			GetVar( nfvdofL, (pvar0[0][idx]).wh, SdQdPtBasL[qd], tcvL );
			double tcvR[NEQ];
			if( ( nbctype==BC_SOLIDSURFACE )||( nbctype==BC_GENERIC1 ) )
			{
				bcvisSolidSurf( tcvL, (pvar0[0][idx]).wh[0], &param, tcvR );
			}
			else if( nbctype==BC_FARFIELD )
			{
				bcFarField( tcvL, (pvar0[0][idx]).wh[0], norm, &param, tcvR );
			}
			else if( nbctype==BC_SYMMETRY )
			{
				bcSymmetry( tcvL, (pvar0[0][idx]).wh[0], norm, tcvR );
			}
			else if( nbctype==BC_INTERBLK )
			{

				int tsR = (psid[0][idx]).lsdidx[0];
				 int  nfvordR = (pvar1[0][idx]).pfv;
				 int  nfvdofR = DOFofOrd2d[nfvordR];
				 int  qdposR = qd;
				 int   nTypeR = ELEMTYPE;
#ifdef QUAD
				 const int * inodeR = (pele1[0][idx]).Node;
				if( inodeR[3]==inodeR[0] ) nTypeR = 3;
#endif
				if( nTypeR==3 )
					GetVar( nfvdofR, (pvar1[0][idx]).wh, SdQdPtBasTri [tsR][nqdpt][qdposR], tcvR );
				else
					GetVar( nfvdofR, (pvar1[0][idx]).wh, SdQdPtBasRect[tsR][nqdpt][qdposR], tcvR );

				double* tnbflux = (pbsesid[0][idx]).flux[nqdpt-1-qd];
#if		AUXVARIABLE==CONSERV
				for(  int  j=0; j<NEQ; j++ )
				{
					(psid[0][idx]).flux[qd][j] = 0.5*(tcvL[j] + tcvR[j]);
					tnbflux[j] = (psid[0][idx]).flux[qd][j];
				}
#elif	AUXVARIABLE==PRIMITI
				double tpvL[NEQ];
				CV2PV( tcvL, (pvar0[0][idx]).wh[0], tpvL );
				double tpvR[NEQ];
				CV2PV( tcvR, (pvar1[0][idx]).wh[0], tpvR );
				tpvL[NEQ-1] /= tpvL[0]*param.R;
				tpvR[NEQ-1] /= tpvR[0]*param.R;
				for(  int  j=0; j<NEQ; j++ )
				{
					(psid[0][idx]).flux[qd][j] = 0.5*(tpvL[j] + tpvR[j]);
					tnbflux[j] = (psid[0][idx]).flux[qd][j];
				}
#endif
				continue;
			}

#if		AUXVARIABLE==CONSERV
			for(  int  j=0; j<NEQ; j++ )
			{
				(psid[0][idx]).flux[qd][j] = tcvR[j];
			}
#elif	AUXVARIABLE==PRIMITI
			double tpvR[NEQ];
			CV2PV( tcvR, tcvR, tpvR );
			tpvR[NEQ-1] /= tpvR[0]*param.R;
			for(  int  j=0; j<NEQ; j++ )
			{
				(psid[0][idx]).flux[qd][j] = tpvR[j];
			}
#endif
			continue;
		}

}
#endif

// host stub function
void op_par_loop_rhscal_auxsideflux2(char const *name, op_set set,
  op_arg arg0,
  op_arg arg1,
  op_arg arg2,
  op_arg arg3,
  op_arg arg4,
  op_arg arg5){

  int nargs = 6;
  op_arg args[6];

  args[0] = arg0;
  args[1] = arg1;
  args[2] = arg2;
  args[3] = arg3;
  args[4] = arg4;
  args[5] = arg5;
  //create aligned pointers for dats
  ALIGNED_side       side * __restrict__ ptr0 = (side *) arg0.data;
  DECLARE_PTR_ALIGNED(ptr0,side_ALIGN);
  ALIGNED_side const side * __restrict__ ptr1 = (side *) arg1.data;
  DECLARE_PTR_ALIGNED(ptr1,side_ALIGN);
  ALIGNED_uvar const uvar * __restrict__ ptr2 = (uvar *) arg2.data;
  DECLARE_PTR_ALIGNED(ptr2,uvar_ALIGN);
  ALIGNED_uvar const uvar * __restrict__ ptr3 = (uvar *) arg3.data;
  DECLARE_PTR_ALIGNED(ptr3,uvar_ALIGN);
  ALIGNED_elem const elem * __restrict__ ptr4 = (elem *) arg4.data;
  DECLARE_PTR_ALIGNED(ptr4,elem_ALIGN);
  ALIGNED_elem const elem * __restrict__ ptr5 = (elem *) arg5.data;
  DECLARE_PTR_ALIGNED(ptr5,elem_ALIGN);

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timing_realloc(6);
  op_timers_core(&cpu_t1, &wall_t1);

  if (OP_diags>2) {
    printf(" kernel routine with indirection: rhscal_auxsideflux2\n");
  }

  int exec_size = op_mpi_halo_exchanges(set, nargs, args);

  if (exec_size >0) {

    #ifdef VECTORIZE
    #pragma novector
    for ( int n=0; n<(exec_size/SIMD_VEC)*SIMD_VEC; n+=SIMD_VEC ){
      if ((n+SIMD_VEC >= set->core_size) && (n+SIMD_VEC-set->core_size < SIMD_VEC)) {
        op_mpi_wait_all(nargs, args);
      }
      ALIGNED_side side dat0[1][SIMD_VEC];
      ALIGNED_side side dat1[1][SIMD_VEC];
      ALIGNED_uvar uvar dat2[1][SIMD_VEC];
      ALIGNED_uvar uvar dat3[1][SIMD_VEC];
      ALIGNED_elem elem dat4[1][SIMD_VEC];
      ALIGNED_elem elem dat5[1][SIMD_VEC];
      #pragma omp simd simdlen(SIMD_VEC)
      for ( int i=0; i<SIMD_VEC; i++ ){
        int idx0_1 = 1 * arg0.map_data[(n+i) * arg0.map->dim + 0];
        int idx1_1 = 1 * arg1.map_data[(n+i) * arg1.map->dim + 0];
        int idx2_1 = 1 * arg2.map_data[(n+i) * arg2.map->dim + 0];
        int idx3_1 = 1 * arg2.map_data[(n+i) * arg2.map->dim + 1];
        int idx4_1 = 1 * arg2.map_data[(n+i) * arg2.map->dim + 0];
        int idx5_1 = 1 * arg2.map_data[(n+i) * arg2.map->dim + 1];

        dat0[0][i] = (ptr0)[idx0_1 + 0];

        dat1[0][i] = (ptr1)[idx1_1 + 0];

        dat2[0][i] = (ptr2)[idx2_1 + 0];

        dat3[0][i] = (ptr3)[idx3_1 + 0];

        dat4[0][i] = (ptr4)[idx4_1 + 0];

        dat5[0][i] = (ptr5)[idx5_1 + 0];

      }
      #pragma omp simd simdlen(SIMD_VEC)
      for ( int i=0; i<SIMD_VEC; i++ ){
        rhscal_auxsideflux2_vec(
          dat0,
          dat1,
          dat2,
          dat3,
          dat4,
          dat5,
          i);
      }
      for ( int i=0; i<SIMD_VEC; i++ ){
        int idx0_1 = 1 * arg0.map_data[(n+i) * arg0.map->dim + 0];

        (ptr0)[idx0_1 + 0] = dat0[0][i];

      }
    }

    //remainder
    for ( int n=(exec_size/SIMD_VEC)*SIMD_VEC; n<exec_size; n++ ){
    #else
    for ( int n=0; n<exec_size; n++ ){
    #endif
      if (n==set->core_size) {
        op_mpi_wait_all(nargs, args);
      }
      int map0idx;
      int map1idx;
      int map2idx;
      int map3idx;
      map0idx = arg0.map_data[n * arg0.map->dim + 0];
      map1idx = arg1.map_data[n * arg1.map->dim + 0];
      map2idx = arg2.map_data[n * arg2.map->dim + 0];
      map3idx = arg2.map_data[n * arg2.map->dim + 1];

      rhscal_auxsideflux2(
        &(ptr0)[1 * map0idx],
        &(ptr1)[1 * map1idx],
        &(ptr2)[1 * map2idx],
        &(ptr3)[1 * map3idx],
        &(ptr4)[1 * map2idx],
        &(ptr5)[1 * map3idx]);
    }
  }

  if (exec_size == 0 || exec_size == set->core_size) {
    op_mpi_wait_all(nargs, args);
  }
  // combine reduction data
  op_mpi_set_dirtybit(nargs, args);

  // update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  OP_kernels[6].name      = name;
  OP_kernels[6].count    += 1;
  OP_kernels[6].time     += wall_t2 - wall_t1;
  OP_kernels[6].transfer += (float)set->size * arg0.size * 2.0f;
  OP_kernels[6].transfer += (float)set->size * arg1.size;
  OP_kernels[6].transfer += (float)set->size * arg2.size;
  OP_kernels[6].transfer += (float)set->size * arg4.size;
  OP_kernels[6].transfer += (float)set->size * arg0.map->dim * 4.0f;
  OP_kernels[6].transfer += (float)set->size * arg1.map->dim * 4.0f;
  OP_kernels[6].transfer += (float)set->size * arg2.map->dim * 4.0f;
}
