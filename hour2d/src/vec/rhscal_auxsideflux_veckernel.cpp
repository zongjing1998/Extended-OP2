//
// auto-generated by op2.py
//

//user function
inline void rhscal_auxsideflux(const uvar *pvar0, const uvar *pvar1, const elem *pele0, const elem *pele1, side *psid)
{
//		int eL  = (*psid).ofElem[1];
		int tsL = (*psid).lsdidx[1];	
//		int eR  = (*psid).ofElem[0];
		int tsR = (*psid).lsdidx[0];	
		
		 int  pfvL    = (*pvar0).pfv;
		 int  nfvdofL = DOFofOrd2d[pfvL];
		 int  pfvR    = (*pvar1).pfv;
		 int  nfvdofR = DOFofOrd2d[pfvR];
		 int   nTypeL = ELEMTYPE;
		 int   nTypeR = ELEMTYPE;
#ifdef QUAD
		 const int * inodeL = (*pele0).Node;
		if( inodeL[3]==inodeL[0] ) nTypeL = 3;
		 const int * inodeR = (*pele1).Node;
		if( inodeR[3]==inodeR[0] ) nTypeR = 3;
#endif
		 int  nqdpt = (*psid).nqdpt;
		double (*SdQdPtBasL)[MAXFVDOF], (*SdQdPtBasR)[MAXFVDOF];
		if( nTypeL==3 ) SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasTri [tsL+3][nqdpt];
		else            SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasRect[tsL+4][nqdpt];
		if( nTypeR==3 ) SdQdPtBasR = (double (*)[MAXFVDOF]) SdQdPtBasTri [tsR  ][nqdpt];
		else            SdQdPtBasR = (double (*)[MAXFVDOF]) SdQdPtBasRect[tsR  ][nqdpt];

		// each gauss quad pts
		for(  int  qd = 0; qd < nqdpt; ++qd )
		{
			double tcvL[NEQ];
			GetVar( nfvdofL, (*pvar0).wh, SdQdPtBasL[qd], tcvL );
			double tcvR[NEQ];
			GetVar( nfvdofR, (*pvar1).wh, SdQdPtBasR[qd], tcvR );

#if		AUXVARIABLE==CONSERV
			for(  int  j=0; j<NEQ; j++ )
			{
				(*psid).flux[qd][j] = 0.5*(tcvL[j] + tcvR[j]);
			}
#elif	AUXVARIABLE==PRIMITI
			double tpvL[NEQ];
			CV2PV( tcvL, (*pvar0).wh[0], tpvL );
			double tpvR[NEQ];
			CV2PV( tcvR, (*pvar1).wh[0], tpvR );
			tpvL[NEQ-1] /= tpvL[0]*param.R;
			tpvR[NEQ-1] /= tpvR[0]*param.R;
			for(  int  j=0; j<NEQ; j++ )
			{
				(*psid).flux[qd][j] = 0.5*(tpvL[j] + tpvR[j]);
			}
#endif
		}// end of qd
}
#ifdef VECTORIZE
//user function -- modified for vectorisation
#if defined __clang__ || defined __GNUC__
__attribute__((always_inline))
#endif
inline void rhscal_auxsideflux_vec( const uvar pvar0[][SIMD_VEC], const uvar pvar1[][SIMD_VEC], const elem pele0[][SIMD_VEC], const elem pele1[][SIMD_VEC], side psid[][SIMD_VEC], int idx ) {

		int tsL = (psid[0][idx]).lsdidx[1];

		int tsR = (psid[0][idx]).lsdidx[0];

		 int  pfvL    = (pvar0[0][idx]).pfv;
		 int  nfvdofL = DOFofOrd2d[pfvL];
		 int  pfvR    = (pvar1[0][idx]).pfv;
		 int  nfvdofR = DOFofOrd2d[pfvR];
		 int   nTypeL = ELEMTYPE;
		 int   nTypeR = ELEMTYPE;
#ifdef QUAD
		 const int * inodeL = (pele0[0][idx]).Node;
		if( inodeL[3]==inodeL[0] ) nTypeL = 3;
		 const int * inodeR = (pele1[0][idx]).Node;
		if( inodeR[3]==inodeR[0] ) nTypeR = 3;
#endif
		 int  nqdpt = (psid[0][idx]).nqdpt;
		double (*SdQdPtBasL)[MAXFVDOF], (*SdQdPtBasR)[MAXFVDOF];
		if( nTypeL==3 ) SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasTri [tsL+3][nqdpt];
		else            SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasRect[tsL+4][nqdpt];
		if( nTypeR==3 ) SdQdPtBasR = (double (*)[MAXFVDOF]) SdQdPtBasTri [tsR  ][nqdpt];
		else            SdQdPtBasR = (double (*)[MAXFVDOF]) SdQdPtBasRect[tsR  ][nqdpt];

		for(  int  qd = 0; qd < nqdpt; ++qd )
		{
			double tcvL[NEQ];
			GetVar( nfvdofL, (pvar0[0][idx]).wh, SdQdPtBasL[qd], tcvL );
			double tcvR[NEQ];
			GetVar( nfvdofR, (pvar1[0][idx]).wh, SdQdPtBasR[qd], tcvR );

#if		AUXVARIABLE==CONSERV
			for(  int  j=0; j<NEQ; j++ )
			{
				(psid[0][idx]).flux[qd][j] = 0.5*(tcvL[j] + tcvR[j]);
			}
#elif	AUXVARIABLE==PRIMITI
			double tpvL[NEQ];
			CV2PV( tcvL, (pvar0[0][idx]).wh[0], tpvL );
			double tpvR[NEQ];
			CV2PV( tcvR, (pvar1[0][idx]).wh[0], tpvR );
			tpvL[NEQ-1] /= tpvL[0]*param.R;
			tpvR[NEQ-1] /= tpvR[0]*param.R;
			for(  int  j=0; j<NEQ; j++ )
			{
				(psid[0][idx]).flux[qd][j] = 0.5*(tpvL[j] + tpvR[j]);
			}
#endif
		}

}
#endif

// host stub function
void op_par_loop_rhscal_auxsideflux(char const *name, op_set set,
  op_arg arg0,
  op_arg arg1,
  op_arg arg2,
  op_arg arg3,
  op_arg arg4){

  int nargs = 5;
  op_arg args[5];

  args[0] = arg0;
  args[1] = arg1;
  args[2] = arg2;
  args[3] = arg3;
  args[4] = arg4;
  //create aligned pointers for dats
  ALIGNED_uvar const uvar * __restrict__ ptr0 = (uvar *) arg0.data;
  DECLARE_PTR_ALIGNED(ptr0,uvar_ALIGN);
  ALIGNED_uvar const uvar * __restrict__ ptr1 = (uvar *) arg1.data;
  DECLARE_PTR_ALIGNED(ptr1,uvar_ALIGN);
  ALIGNED_elem const elem * __restrict__ ptr2 = (elem *) arg2.data;
  DECLARE_PTR_ALIGNED(ptr2,elem_ALIGN);
  ALIGNED_elem const elem * __restrict__ ptr3 = (elem *) arg3.data;
  DECLARE_PTR_ALIGNED(ptr3,elem_ALIGN);
  ALIGNED_side       side * __restrict__ ptr4 = (side *) arg4.data;
  DECLARE_PTR_ALIGNED(ptr4,side_ALIGN);

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timing_realloc(5);
  op_timers_core(&cpu_t1, &wall_t1);

  if (OP_diags>2) {
    printf(" kernel routine with indirection: rhscal_auxsideflux\n");
  }

  int exec_size = op_mpi_halo_exchanges(set, nargs, args);

  if (exec_size >0) {

    #ifdef VECTORIZE
    #pragma novector
    for ( int n=0; n<(exec_size/SIMD_VEC)*SIMD_VEC; n+=SIMD_VEC ){
      if ((n+SIMD_VEC >= set->core_size) && (n+SIMD_VEC-set->core_size < SIMD_VEC)) {
        op_mpi_wait_all(nargs, args);
      }
      ALIGNED_uvar uvar dat0[1][SIMD_VEC];
      ALIGNED_uvar uvar dat1[1][SIMD_VEC];
      ALIGNED_elem elem dat2[1][SIMD_VEC];
      ALIGNED_elem elem dat3[1][SIMD_VEC];
      ALIGNED_side side dat4[1][SIMD_VEC];
      #pragma omp simd simdlen(SIMD_VEC)
      for ( int i=0; i<SIMD_VEC; i++ ){
        int idx0_1 = 1 * arg0.map_data[(n+i) * arg0.map->dim + 0];
        int idx1_1 = 1 * arg0.map_data[(n+i) * arg0.map->dim + 1];
        int idx2_1 = 1 * arg0.map_data[(n+i) * arg0.map->dim + 0];
        int idx3_1 = 1 * arg0.map_data[(n+i) * arg0.map->dim + 1];
        int idx4_1 = 1 * arg4.map_data[(n+i) * arg4.map->dim + 0];

        dat0[0][i] = (ptr0)[idx0_1 + 0];

        dat1[0][i] = (ptr1)[idx1_1 + 0];

        dat2[0][i] = (ptr2)[idx2_1 + 0];

        dat3[0][i] = (ptr3)[idx3_1 + 0];

        dat4[0][i] = (ptr4)[idx4_1 + 0];

      }
      #pragma omp simd simdlen(SIMD_VEC)
      for ( int i=0; i<SIMD_VEC; i++ ){
        rhscal_auxsideflux_vec(
          dat0,
          dat1,
          dat2,
          dat3,
          dat4,
          i);
      }
      for ( int i=0; i<SIMD_VEC; i++ ){
        int idx4_1 = 1 * arg4.map_data[(n+i) * arg4.map->dim + 0];

        (ptr4)[idx4_1 + 0] = dat4[0][i];

      }
    }

    //remainder
    for ( int n=(exec_size/SIMD_VEC)*SIMD_VEC; n<exec_size; n++ ){
    #else
    for ( int n=0; n<exec_size; n++ ){
    #endif
      if (n==set->core_size) {
        op_mpi_wait_all(nargs, args);
      }
      int map0idx;
      int map1idx;
      int map4idx;
      map0idx = arg0.map_data[n * arg0.map->dim + 0];
      map1idx = arg0.map_data[n * arg0.map->dim + 1];
      map4idx = arg4.map_data[n * arg4.map->dim + 0];

      rhscal_auxsideflux(
        &(ptr0)[1 * map0idx],
        &(ptr1)[1 * map1idx],
        &(ptr2)[1 * map0idx],
        &(ptr3)[1 * map1idx],
        &(ptr4)[1 * map4idx]);
    }
  }

  if (exec_size == 0 || exec_size == set->core_size) {
    op_mpi_wait_all(nargs, args);
  }
  // combine reduction data
  op_mpi_set_dirtybit(nargs, args);

  // update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  OP_kernels[5].name      = name;
  OP_kernels[5].count    += 1;
  OP_kernels[5].time     += wall_t2 - wall_t1;
  OP_kernels[5].transfer += (float)set->size * arg0.size;
  OP_kernels[5].transfer += (float)set->size * arg2.size;
  OP_kernels[5].transfer += (float)set->size * arg4.size * 2.0f;
  OP_kernels[5].transfer += (float)set->size * arg0.map->dim * 4.0f;
  OP_kernels[5].transfer += (float)set->size * arg4.map->dim * 4.0f;
}
