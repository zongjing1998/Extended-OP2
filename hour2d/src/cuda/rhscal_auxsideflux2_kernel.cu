//
// auto-generated by op2.py
//

//user function
__device__ void rhscal_auxsideflux2_gpu( side *psid, side *pbsesid, const uvar *pvar0, const uvar *pvar1, const elem *pele0, const elem *pele1) {
		int bceR = (*psid).ofElem[0];
		if( bceR >= 0 )
		{
			printf( "calauxsideflux bc error!\n" );

		}
		 int  nbctype = GetBCType( bceR );

		int tsL = (*psid).lsdidx[1];
		 int  nfvordL = (*pvar0).pfv;
		 int  nfvdofL = DOFofOrd2d_cuda[nfvordL];
		 int   nTypeL = ELEMTYPE;
#ifdef QUAD
		 const int * inodeL = (*pele0).Node;
		if( inodeL[3]==inodeL[0] ) nTypeL = 3;
#endif
		double norm[NDIM];
		for(  int  nd=0; nd<NDIM; nd++ )
			norm[nd] = (*psid).norm[nd];
		 int  nqdpt = (*psid).nqdpt;
		double (*SdQdPtBasL)[MAXFVDOF];
		if( nTypeL==3 ) SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasTri_cuda [tsL+3][nqdpt];
		else            SdQdPtBasL = (double (*)[MAXFVDOF]) SdQdPtBasRect_cuda[tsL+4][nqdpt];

		for(  int  qd = 0; qd < nqdpt; ++qd )
		{
			double tcvL[NEQ];
			GetVar_gpu( nfvdofL, (*pvar0).wh, SdQdPtBasL[qd], tcvL );
			double tcvR[NEQ];
			if( ( nbctype==BC_SOLIDSURFACE )||( nbctype==BC_GENERIC1 ) )
			{
				bcvisSolidSurf_gpu( tcvL, (*pvar0).wh[0], &param_cuda, tcvR );
			}
			else if( nbctype==BC_FARFIELD )
			{
				bcFarField_gpu( tcvL, (*pvar0).wh[0], norm, &param_cuda, tcvR );
			}
			else if( nbctype==BC_SYMMETRY )
			{
				bcSymmetry_gpu( tcvL, (*pvar0).wh[0], norm, tcvR );
			}
			else if( nbctype==BC_INTERBLK )
			{

				int tsR = (*psid).lsdidx[0];
				 int  nfvordR = (*pvar1).pfv;
				 int  nfvdofR = DOFofOrd2d_cuda[nfvordR];
				 int  qdposR = qd;
				 int   nTypeR = ELEMTYPE;
#ifdef QUAD
				 const int * inodeR = (*pele1).Node;
				if( inodeR[3]==inodeR[0] ) nTypeR = 3;
#endif
				if( nTypeR==3 )
					GetVar_gpu( nfvdofR, (*pvar1).wh, SdQdPtBasTri_cuda [tsR][nqdpt][qdposR], tcvR );
				else
					GetVar_gpu( nfvdofR, (*pvar1).wh, SdQdPtBasRect_cuda[tsR][nqdpt][qdposR], tcvR );

				double* tnbflux = (*pbsesid).flux[nqdpt-1-qd];
#if		AUXVARIABLE==CONSERV
				for(  int  j=0; j<NEQ; j++ )
				{
					(*psid).flux[qd][j] = 0.5*(tcvL[j] + tcvR[j]);
					tnbflux[j] = (*psid).flux[qd][j];
				}
#elif	AUXVARIABLE==PRIMITI
				double tpvL[NEQ];
				CV2PV_gpu( tcvL, (*pvar0).wh[0], tpvL );
				double tpvR[NEQ];
				CV2PV_gpu( tcvR, (*pvar1).wh[0], tpvR );
				tpvL[NEQ-1] /= tpvL[0]*param_cuda.R;
				tpvR[NEQ-1] /= tpvR[0]*param_cuda.R;
				for(  int  j=0; j<NEQ; j++ )
				{
					(*psid).flux[qd][j] = 0.5*(tpvL[j] + tpvR[j]);
					tnbflux[j] = (*psid).flux[qd][j];
				}
#endif
				continue;
			}

#if		AUXVARIABLE==CONSERV
			for(  int  j=0; j<NEQ; j++ )
			{
				(*psid).flux[qd][j] = tcvR[j];
			}
#elif	AUXVARIABLE==PRIMITI
			double tpvR[NEQ];
			CV2PV_gpu( tcvR, tcvR, tpvR );
			tpvR[NEQ-1] /= tpvR[0]*param_cuda.R;
			for(  int  j=0; j<NEQ; j++ )
			{
				(*psid).flux[qd][j] = tpvR[j];
			}
#endif
			continue;
		}

}

// CUDA kernel function
__global__ void op_cuda_rhscal_auxsideflux2(
  side *__restrict ind_arg0,
  side *__restrict ind_arg1,
  const uvar *__restrict ind_arg2,
  const elem *__restrict ind_arg3,
  const int *__restrict opDat0Map,
  const int *__restrict opDat1Map,
  const int *__restrict opDat2Map,
  int start,
  int end,
  int *col_reord,
  int   set_size) {
  int tid = threadIdx.x + blockIdx.x * blockDim.x;
  if (tid + start < end) {
    int n = col_reord[tid + start];
    //initialise local variables
    int map0idx;
    int map1idx;
    int map2idx;
    int map3idx;
    map0idx = opDat0Map[n + set_size * 0];
    map1idx = opDat1Map[n + set_size * 0];
    map2idx = opDat2Map[n + set_size * 0];
    map3idx = opDat2Map[n + set_size * 1];

    //user-supplied kernel call
    rhscal_auxsideflux2_gpu(ind_arg0+map0idx*1,
                        ind_arg1+map1idx*1,
                        ind_arg2+map2idx*1,
                        ind_arg2+map3idx*1,
                        ind_arg3+map2idx*1,
                        ind_arg3+map3idx*1);
  }
}


//host stub function
void op_par_loop_rhscal_auxsideflux2(char const *name, op_set set,
  op_arg arg0,
  op_arg arg1,
  op_arg arg2,
  op_arg arg3,
  op_arg arg4,
  op_arg arg5){

  int nargs = 6;
  op_arg args[6];

  args[0] = arg0;
  args[1] = arg1;
  args[2] = arg2;
  args[3] = arg3;
  args[4] = arg4;
  args[5] = arg5;

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timing_realloc(6);
  op_timers_core(&cpu_t1, &wall_t1);
  OP_kernels[6].name      = name;
  OP_kernels[6].count    += 1;


  int    ninds   = 4;
  int    inds[6] = {0,1,2,2,3,3};

  if (OP_diags>2) {
    printf(" kernel routine with indirection: rhscal_auxsideflux2\n");
  }

  //get plan
  #ifdef OP_PART_SIZE_6
    int part_size = OP_PART_SIZE_6;
  #else
    int part_size = OP_part_size;
  #endif

  int set_size = op_mpi_halo_exchanges_grouped(set, nargs, args, 2);
  if (set_size > 0) {

    op_plan *Plan = op_plan_get_stage(name,set,part_size,nargs,args,ninds,inds,OP_COLOR2);

    //execute plan
    for ( int col=0; col<Plan->ncolors; col++ ){
      if (col==Plan->ncolors_core) {
        op_mpi_wait_all_grouped(nargs, args, 2);
      }
      #ifdef OP_BLOCK_SIZE_6
      int nthread = OP_BLOCK_SIZE_6;
      #else
      int nthread = OP_block_size;
      #endif

      int start = Plan->col_offsets[0][col];
      int end = Plan->col_offsets[0][col+1];
      int nblocks = (end - start - 1)/nthread + 1;
      op_cuda_rhscal_auxsideflux2<<<nblocks,nthread>>>(
      (side *)arg0.data_d,
      (side *)arg1.data_d,
      (uvar *)arg2.data_d,
      (elem *)arg4.data_d,
      arg0.map_data_d,
      arg1.map_data_d,
      arg2.map_data_d,
      start,
      end,
      Plan->col_reord,
      set->size+set->exec_size);

    }
    OP_kernels[6].transfer  += Plan->transfer;
    OP_kernels[6].transfer2 += Plan->transfer2;
  }
  op_mpi_set_dirtybit_cuda(nargs, args);
  cutilSafeCall(cudaDeviceSynchronize());
  //update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  OP_kernels[6].time     += wall_t2 - wall_t1;
}
